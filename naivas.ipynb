{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd    \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from datetime import datetime \n",
    "import time  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Client Object\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'adrianjuliusaluoch.json'\n",
    "client = bigquery.Client(project='project-adrian-julius-aluoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Food Cupboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium Firefox WebDriver\n",
    "options = Options()\n",
    "options.headless = True  # Run Firefox in headless mode, without opening a browser window\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Define Links\n",
    "links = [\n",
    "        'food-additives',\n",
    "        'snacks',\n",
    "        'fats-oils',\n",
    "        'breakfast',\n",
    "        'commodities',\n",
    "        'canned-frozen-meals',\n",
    "        'naivas-dry-cereals-nuts'\n",
    "    ]\n",
    "\n",
    "# Define Loop\n",
    "for link in links:\n",
    "    # Specify Website URL\n",
    "    url = 'https://naivas.online/' + str(link)\n",
    "\n",
    "    # Load Webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for Page to fully render\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    # Get Initial Page Height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll Down to Page Bottom\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)  # Wait 4 seconds for page to load more content\n",
    "\n",
    "        # Compare Final Scroll Height with Initial Scroll Height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # If Height is constant, exit loop\n",
    "        last_height = new_height\n",
    "\n",
    "        # Limit Number of Listings\n",
    "        if len(BeautifulSoup(driver.page_source, 'html.parser').find_all('div', class_='h-full')) >= 500:\n",
    "            break\n",
    "\n",
    "    # Get Page Source after JavaScript Rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    groceries = soup.find_all('div', class_='h-full')\n",
    "\n",
    "    # Extract grocery Listings from Jiji\n",
    "    bigdata = pd.DataFrame()\n",
    "    for grocery in groceries:\n",
    "        try:\n",
    "            product_name = grocery.find('span',class_='line-clamp-2 text-ellipsis').text.strip()\n",
    "            product_category = 'Food Cupboard'\n",
    "            product_subcategory = link\n",
    "            try:\n",
    "                product_availability = grocery.find('p',class_='text-naivas-red text-sm font-bold').text.strip()\n",
    "            except Exception as e:\n",
    "                product_availability = np.nan\n",
    "            try:\n",
    "                final_price = grocery.find('span',class_='font-bold text-naivas-green mb-1 md:mb-0 pe-2').text.strip()\n",
    "            except Exception as e:\n",
    "                final_price = np.nan\n",
    "            last_scraped = datetime.now()\n",
    "            try:\n",
    "                initial_price = grocery.find('span',class_='text-red-600 text-xs line-through font-light').text.strip()\n",
    "            except Exception as e:\n",
    "                initial_price = np.nan\n",
    "\n",
    "            # Append Data to DataFrame\n",
    "            data = pd.DataFrame({\n",
    "                    'product_name':[product_name],\n",
    "                    'product_category':[product_category],\n",
    "                    'product_subcategory':[product_subcategory],\n",
    "                    'product_availability':[product_availability],\n",
    "                    'initial_price':[initial_price],\n",
    "                    'final_price':[final_price],\n",
    "                    'last_scraped':[last_scraped]\n",
    "                })\n",
    "\n",
    "            bigdata = pd.concat([bigdata,data])\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Handle Database Import Error\n",
    "    bigdata.to_csv('naivas_metadata.csv',index=False)\n",
    "    bigdata = pd.read_csv('naivas_metadata.csv') \n",
    "    bigdata['last_scraped'] = pd.to_datetime(bigdata['last_scraped'])\n",
    "\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(bigdata, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Define SQL Query to Retrieve Open Weather Data from Google Cloud BigQuery\n",
    "    sql = (\n",
    "           'SELECT *'\n",
    "           'FROM `cronjobs.naivas`'\n",
    "           )\n",
    "    \n",
    "    # Run SQL Query\n",
    "    data = client.query(sql).to_dataframe()\n",
    "\n",
    "    # Delete Original Table\n",
    "    client.delete_table(table_id)\n",
    "    print(f\"Table deleted successfully.\")\n",
    "\n",
    "    # Check Total Number of Duplicate Records\n",
    "    duplicated = data.duplicated(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price']).sum()\n",
    "    \n",
    "    # Remove Duplicate Records\n",
    "    data.drop_duplicates(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price'],inplace=True)\n",
    "\n",
    "    # Define the dataset ID and table ID\n",
    "    dataset_id = 'cronjobs'\n",
    "    table_id = 'naivas'\n",
    "    \n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_category\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_subcategory\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_availability\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"initial_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"final_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"last_scraped\", \"TIMESTAMP\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Create the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    table = client.create_table(table)\n",
    "    \n",
    "    print(f\"Table {table.table_id} created successfully.\")\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(data, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Success\n",
    "    print(f\"Data has been successfully retrieved, saved, and appended to the BigQuery table for {link}.\")\n",
    "\n",
    "    # Remove Temporary File\n",
    "    os.remove('naivas_metadata.csv')\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Fresh Food**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium Firefox WebDriver\n",
    "options = Options()\n",
    "options.headless = True  # Run Firefox in headless mode, without opening a browser window\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Define Links\n",
    "links = [\n",
    "        'naivas-bakery',\n",
    "        'fruit-veggie',\n",
    "        'dairy',\n",
    "        'pre-packed-meat-products',\n",
    "        'cold-deli',\n",
    "        'naivas-butchery',\n",
    "        'outsourced-bakery'\n",
    "    ]\n",
    "\n",
    "# Define Loop\n",
    "for link in links:\n",
    "    # Specify Website URL\n",
    "    url = 'https://naivas.online/' + str(link)\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load Webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for Page to fully render\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    # Get Initial Page Height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll Down to Page Bottom\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)  # Wait 4 seconds for page to load more content\n",
    "\n",
    "        # Compare Final Scroll Height with Initial Scroll Height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # If Height is constant, exit loop\n",
    "        last_height = new_height\n",
    "\n",
    "        # Limit Number of Listings\n",
    "        if len(BeautifulSoup(driver.page_source, 'html.parser').find_all('div', class_='h-full')) >= 500:\n",
    "            break\n",
    "\n",
    "    # Get Page Source after JavaScript Rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    groceries = soup.find_all('div', class_='h-full')\n",
    "\n",
    "    # Extract grocery Listings from Jiji\n",
    "    bigdata = pd.DataFrame()\n",
    "    for grocery in groceries:\n",
    "        try:\n",
    "            product_name = grocery.find('span',class_='line-clamp-2 text-ellipsis').text.strip()\n",
    "            product_category = 'Fresh Food'\n",
    "            product_subcategory = link\n",
    "            try:\n",
    "                product_availability = grocery.find('p',class_='text-naivas-red text-sm font-bold').text.strip()\n",
    "            except Exception as e:\n",
    "                product_availability = np.nan\n",
    "            try:\n",
    "                final_price = grocery.find('span',class_='font-bold text-naivas-green mb-1 md:mb-0 pe-2').text.strip()\n",
    "            except Exception as e:\n",
    "                final_price = np.nan\n",
    "            last_scraped = datetime.now()\n",
    "            try:\n",
    "                initial_price = grocery.find('span',class_='text-red-600 text-xs line-through font-light').text.strip()\n",
    "            except Exception as e:\n",
    "                initial_price = np.nan\n",
    "\n",
    "            # Append Data to DataFrame\n",
    "            data = pd.DataFrame({\n",
    "                    'product_name':[product_name],\n",
    "                    'product_category':[product_category],\n",
    "                    'product_subcategory':[product_subcategory],\n",
    "                    'product_availability':[product_availability],\n",
    "                    'initial_price':[initial_price],\n",
    "                    'final_price':[final_price],\n",
    "                    'last_scraped':[last_scraped]\n",
    "                })\n",
    "\n",
    "            bigdata = pd.concat([bigdata,data])\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Handle Database Import Error\n",
    "    bigdata.to_csv('naivas_metadata.csv',index=False)\n",
    "    bigdata = pd.read_csv('naivas_metadata.csv')\n",
    "\n",
    "    # Drop Duplicate Records\n",
    "    bigdata['last_scraped'] = pd.to_datetime(bigdata['last_scraped'])\n",
    "\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(bigdata, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Define SQL Query to Retrieve Open Weather Data from Google Cloud BigQuery\n",
    "    sql = (\n",
    "           'SELECT *'\n",
    "           'FROM `cronjobs.naivas`'\n",
    "           )\n",
    "    \n",
    "    # Run SQL Query\n",
    "    data = client.query(sql).to_dataframe()\n",
    "\n",
    "    # Delete Original Table\n",
    "    client.delete_table(table_id)\n",
    "    print(f\"Table deleted successfully.\")\n",
    "\n",
    "    # Check Total Number of Duplicate Records\n",
    "    duplicated = data.duplicated(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price']).sum()\n",
    "    \n",
    "    # Remove Duplicate Records\n",
    "    data.drop_duplicates(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price'],inplace=True)\n",
    "\n",
    "    # Define the dataset ID and table ID\n",
    "    dataset_id = 'cronjobs'\n",
    "    table_id = 'naivas'\n",
    "    \n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_category\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_subcategory\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_availability\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"initial_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"final_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"last_scraped\", \"TIMESTAMP\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Create the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    table = client.create_table(table)\n",
    "    \n",
    "    print(f\"Table {table.table_id} created successfully.\")\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(data, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Success\n",
    "    print(f\"Data has been successfully retrieved, saved, and appended to the BigQuery table for {link}\")\n",
    "\n",
    "    # Remove Temporary File\n",
    "    os.remove('naivas_metadata.csv')\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Baby & Kids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium Firefox WebDriver\n",
    "options = Options()\n",
    "options.headless = True  # Run Firefox in headless mode, without opening a browser window\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Define Links\n",
    "links = [\n",
    "        'nursing-feeding',\n",
    "        'baby-hygiene',\n",
    "        'baby-skincare'\n",
    "    ]\n",
    "\n",
    "# Define Loop\n",
    "for link in links:\n",
    "    # Specify Website URL\n",
    "    url = 'https://naivas.online/' + str(link)\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load Webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for Page to fully render\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    # Get Initial Page Height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll Down to Page Bottom\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)  # Wait 4 seconds for page to load more content\n",
    "\n",
    "        # Compare Final Scroll Height with Initial Scroll Height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # If Height is constant, exit loop\n",
    "        last_height = new_height\n",
    "\n",
    "        # Limit Number of Listings\n",
    "        if len(BeautifulSoup(driver.page_source, 'html.parser').find_all('div', class_='h-full')) >= 500:\n",
    "            break\n",
    "\n",
    "    # Get Page Source after JavaScript Rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    groceries = soup.find_all('div', class_='h-full')\n",
    "\n",
    "    # Extract grocery Listings from Jiji\n",
    "    bigdata = pd.DataFrame()\n",
    "    for grocery in groceries:\n",
    "        try:\n",
    "            product_name = grocery.find('span',class_='line-clamp-2 text-ellipsis').text.strip()\n",
    "            product_category = 'Baby & Kids'\n",
    "            product_subcategory = link\n",
    "            try:\n",
    "                product_availability = grocery.find('p',class_='text-naivas-red text-sm font-bold').text.strip()\n",
    "            except Exception as e:\n",
    "                product_availability = np.nan\n",
    "            try:\n",
    "                final_price = grocery.find('span',class_='font-bold text-naivas-green mb-1 md:mb-0 pe-2').text.strip()\n",
    "            except Exception as e:\n",
    "                final_price = np.nan\n",
    "            last_scraped = datetime.now()\n",
    "            try:\n",
    "                initial_price = grocery.find('span',class_='text-red-600 text-xs line-through font-light').text.strip()\n",
    "            except Exception as e:\n",
    "                initial_price = np.nan\n",
    "\n",
    "            # Append Data to DataFrame\n",
    "            data = pd.DataFrame({\n",
    "                    'product_name':[product_name],\n",
    "                    'product_category':[product_category],\n",
    "                    'product_subcategory':[product_subcategory],\n",
    "                    'product_availability':[product_availability],\n",
    "                    'initial_price':[initial_price],\n",
    "                    'final_price':[final_price],\n",
    "                    'last_scraped':[last_scraped]\n",
    "                })\n",
    "\n",
    "            bigdata = pd.concat([bigdata,data])\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Handle Database Import Error\n",
    "    bigdata.to_csv('naivas_metadata.csv',index=False)\n",
    "    bigdata = pd.read_csv('naivas_metadata.csv')\n",
    "\n",
    "    # Drop Duplicate Records\n",
    "    bigdata['last_scraped'] = pd.to_datetime(bigdata['last_scraped'])\n",
    "\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(bigdata, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Define SQL Query to Retrieve Open Weather Data from Google Cloud BigQuery\n",
    "    sql = (\n",
    "           'SELECT *'\n",
    "           'FROM `cronjobs.naivas`'\n",
    "           )\n",
    "    \n",
    "    # Run SQL Query\n",
    "    data = client.query(sql).to_dataframe()\n",
    "\n",
    "    # Delete Original Table\n",
    "    client.delete_table(table_id)\n",
    "    print(f\"Table deleted successfully.\")\n",
    "\n",
    "    # Check Total Number of Duplicate Records\n",
    "    duplicated = data.duplicated(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price']).sum()\n",
    "    \n",
    "    # Remove Duplicate Records\n",
    "    data.drop_duplicates(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price'],inplace=True)\n",
    "\n",
    "    # Define the dataset ID and table ID\n",
    "    dataset_id = 'cronjobs'\n",
    "    table_id = 'naivas'\n",
    "    \n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_category\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_subcategory\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_availability\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"initial_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"final_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"last_scraped\", \"TIMESTAMP\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Create the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    table = client.create_table(table)\n",
    "    \n",
    "    print(f\"Table {table.table_id} created successfully.\")\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(data, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Success\n",
    "    print(f\"Data has been successfully retrieved, saved, and appended to the BigQuery table for {link}\")\n",
    "\n",
    "    # Remove Temporary File\n",
    "    os.remove('naivas_metadata.csv')\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Electronics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium Firefox WebDriver\n",
    "options = Options()\n",
    "options.headless = True  # Run Firefox in headless mode, without opening a browser window\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Define Links\n",
    "links = [\n",
    "        'kitchen-appliances',\n",
    "        'garment-care',\n",
    "        'fridges-freezers',\n",
    "        'cookers',\n",
    "        'air-conditioning',\n",
    "        'sound-system',\n",
    "        'televisions',\n",
    "        'washing-machine'\n",
    "    ]\n",
    "\n",
    "# Define Loop\n",
    "for link in links:\n",
    "    # Specify Website URL\n",
    "    url = 'https://naivas.online/' + str(link)\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load Webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for Page to fully render\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    # Get Initial Page Height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll Down to Page Bottom\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)  # Wait 4 seconds for page to load more content\n",
    "\n",
    "        # Compare Final Scroll Height with Initial Scroll Height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # If Height is constant, exit loop\n",
    "        last_height = new_height\n",
    "\n",
    "        # Limit Number of Listings\n",
    "        if len(BeautifulSoup(driver.page_source, 'html.parser').find_all('div', class_='h-full')) >= 500:\n",
    "            break\n",
    "\n",
    "    # Get Page Source after JavaScript Rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    groceries = soup.find_all('div', class_='h-full')\n",
    "\n",
    "    # Extract grocery Listings from Jiji\n",
    "    bigdata = pd.DataFrame()\n",
    "    for grocery in groceries:\n",
    "        try:\n",
    "            product_name = grocery.find('span',class_='line-clamp-2 text-ellipsis').text.strip()\n",
    "            product_category = 'Electronics'\n",
    "            product_subcategory = link\n",
    "            try:\n",
    "                product_availability = grocery.find('p',class_='text-naivas-red text-sm font-bold').text.strip()\n",
    "            except Exception as e:\n",
    "                product_availability = np.nan\n",
    "            try:\n",
    "                final_price = grocery.find('span',class_='font-bold text-naivas-green mb-1 md:mb-0 pe-2').text.strip()\n",
    "            except Exception as e:\n",
    "                final_price = np.nan\n",
    "            last_scraped = datetime.now()\n",
    "            try:\n",
    "                initial_price = grocery.find('span',class_='text-red-600 text-xs line-through font-light').text.strip()\n",
    "            except Exception as e:\n",
    "                initial_price = np.nan\n",
    "\n",
    "            # Append Data to DataFrame\n",
    "            data = pd.DataFrame({\n",
    "                    'product_name':[product_name],\n",
    "                    'product_category':[product_category],\n",
    "                    'product_subcategory':[product_subcategory],\n",
    "                    'product_availability':[product_availability],\n",
    "                    'initial_price':[initial_price],\n",
    "                    'final_price':[final_price],\n",
    "                    'last_scraped':[last_scraped]\n",
    "                })\n",
    "\n",
    "            bigdata = pd.concat([bigdata,data])\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Handle Database Import Error\n",
    "    bigdata.to_csv('naivas_metadata.csv',index=False)\n",
    "    bigdata = pd.read_csv('naivas_metadata.csv')\n",
    "\n",
    "    # Drop Duplicate Records\n",
    "    bigdata['last_scraped'] = pd.to_datetime(bigdata['last_scraped'])\n",
    "\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(bigdata, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Define SQL Query to Retrieve Open Weather Data from Google Cloud BigQuery\n",
    "    sql = (\n",
    "           'SELECT *'\n",
    "           'FROM `cronjobs.naivas`'\n",
    "           )\n",
    "    \n",
    "    # Run SQL Query\n",
    "    data = client.query(sql).to_dataframe()\n",
    "\n",
    "    # Delete Original Table\n",
    "    client.delete_table(table_id)\n",
    "    print(f\"Table deleted successfully.\")\n",
    "\n",
    "    # Check Total Number of Duplicate Records\n",
    "    duplicated = data.duplicated(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price']).sum()\n",
    "    \n",
    "    # Remove Duplicate Records\n",
    "    data.drop_duplicates(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price'],inplace=True)\n",
    "\n",
    "    # Define the dataset ID and table ID\n",
    "    dataset_id = 'cronjobs'\n",
    "    table_id = 'naivas'\n",
    "    \n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_category\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_subcategory\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_availability\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"initial_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"final_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"last_scraped\", \"TIMESTAMP\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Create the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    table = client.create_table(table)\n",
    "    \n",
    "    print(f\"Table {table.table_id} created successfully.\")\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(data, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Success\n",
    "    print(f\"Data has been successfully retrieved, saved, and appended to the BigQuery table for {link}\")\n",
    "\n",
    "    # Remove Temporary File\n",
    "    os.remove('naivas_metadata.csv')\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium Firefox WebDriver\n",
    "options = Options()\n",
    "options.headless = True  # Run Firefox in headless mode, without opening a browser window\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Define Links\n",
    "links = [\n",
    "        'laundry',\n",
    "        'dish-washing',\n",
    "        'surface-cleaners-protection',\n",
    "        'all-purpose-cleaners',\n",
    "        'toilet-septic-care'\n",
    "    ]\n",
    "\n",
    "# Define Loop\n",
    "for link in links:\n",
    "    # Specify Website URL\n",
    "    url = 'https://naivas.online/' + str(link)\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load Webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for Page to fully render\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    # Get Initial Page Height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll Down to Page Bottom\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)  # Wait 4 seconds for page to load more content\n",
    "\n",
    "        # Compare Final Scroll Height with Initial Scroll Height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # If Height is constant, exit loop\n",
    "        last_height = new_height\n",
    "\n",
    "        # Limit Number of Listings\n",
    "        if len(BeautifulSoup(driver.page_source, 'html.parser').find_all('div', class_='h-full')) >= 500:\n",
    "            break\n",
    "\n",
    "    # Get Page Source after JavaScript Rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    groceries = soup.find_all('div', class_='h-full')\n",
    "\n",
    "    # Extract grocery Listings from Jiji\n",
    "    bigdata = pd.DataFrame()\n",
    "    for grocery in groceries:\n",
    "        try:\n",
    "            product_name = grocery.find('span',class_='line-clamp-2 text-ellipsis').text.strip()\n",
    "            product_category = 'Cleaning'\n",
    "            product_subcategory = link\n",
    "            try:\n",
    "                product_availability = grocery.find('p',class_='text-naivas-red text-sm font-bold').text.strip()\n",
    "            except Exception as e:\n",
    "                product_availability = np.nan\n",
    "            try:\n",
    "                final_price = grocery.find('span',class_='font-bold text-naivas-green mb-1 md:mb-0 pe-2').text.strip()\n",
    "            except Exception as e:\n",
    "                final_price = np.nan\n",
    "            last_scraped = datetime.now()\n",
    "            try:\n",
    "                initial_price = grocery.find('span',class_='text-red-600 text-xs line-through font-light').text.strip()\n",
    "            except Exception as e:\n",
    "                initial_price = np.nan\n",
    "\n",
    "            # Append Data to DataFrame\n",
    "            data = pd.DataFrame({\n",
    "                    'product_name':[product_name],\n",
    "                    'product_category':[product_category],\n",
    "                    'product_subcategory':[product_subcategory],\n",
    "                    'product_availability':[product_availability],\n",
    "                    'initial_price':[initial_price],\n",
    "                    'final_price':[final_price],\n",
    "                    'last_scraped':[last_scraped]\n",
    "                })\n",
    "\n",
    "            bigdata = pd.concat([bigdata,data])\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Handle Database Import Error\n",
    "    bigdata.to_csv('naivas_metadata.csv',index=False)\n",
    "    bigdata = pd.read_csv('naivas_metadata.csv')\n",
    "\n",
    "    # Drop Duplicate Records\n",
    "    bigdata['last_scraped'] = pd.to_datetime(bigdata['last_scraped'])\n",
    "\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(bigdata, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Define SQL Query to Retrieve Open Weather Data from Google Cloud BigQuery\n",
    "    sql = (\n",
    "           'SELECT *'\n",
    "           'FROM `cronjobs.naivas`'\n",
    "           )\n",
    "    \n",
    "    # Run SQL Query\n",
    "    data = client.query(sql).to_dataframe()\n",
    "\n",
    "    # Delete Original Table\n",
    "    client.delete_table(table_id)\n",
    "    print(f\"Table deleted successfully.\")\n",
    "\n",
    "    # Check Total Number of Duplicate Records\n",
    "    duplicated = data.duplicated(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price']).sum()\n",
    "    \n",
    "    # Remove Duplicate Records\n",
    "    data.drop_duplicates(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price'],inplace=True)\n",
    "\n",
    "    # Define the dataset ID and table ID\n",
    "    dataset_id = 'cronjobs'\n",
    "    table_id = 'naivas'\n",
    "    \n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_category\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_subcategory\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_availability\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"initial_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"final_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"last_scraped\", \"TIMESTAMP\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Create the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    table = client.create_table(table)\n",
    "    \n",
    "    print(f\"Table {table.table_id} created successfully.\")\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(data, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Success\n",
    "    print(f\"Data has been successfully retrieved, saved, and appended to the BigQuery table for {link}\")\n",
    "\n",
    "    # Remove Temporary File\n",
    "    os.remove('naivas_metadata.csv')\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Beauty & Cosmetics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium Firefox WebDriver\n",
    "options = Options()\n",
    "options.headless = True  # Run Firefox in headless mode, without opening a browser window\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Define Links\n",
    "links = [\n",
    "        'health-wellness',\n",
    "        'personal-care',\n",
    "        'body-care',\n",
    "        'fragrances',\n",
    "        'haircare-styling',\n",
    "        'face-care'\n",
    "    ]\n",
    "\n",
    "# Define Loop\n",
    "for link in links:\n",
    "    # Specify Website URL\n",
    "    url = 'https://naivas.online/' + str(link)\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load Webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for Page to fully render\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    # Get Initial Page Height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll Down to Page Bottom\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)  # Wait 4 seconds for page to load more content\n",
    "\n",
    "        # Compare Final Scroll Height with Initial Scroll Height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # If Height is constant, exit loop\n",
    "        last_height = new_height\n",
    "\n",
    "        # Limit Number of Listings\n",
    "        if len(BeautifulSoup(driver.page_source, 'html.parser').find_all('div', class_='h-full')) >= 500:\n",
    "            break\n",
    "\n",
    "    # Get Page Source after JavaScript Rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    groceries = soup.find_all('div', class_='h-full')\n",
    "\n",
    "    # Extract grocery Listings from Jiji\n",
    "    bigdata = pd.DataFrame()\n",
    "    for grocery in groceries:\n",
    "        try:\n",
    "            product_name = grocery.find('span',class_='line-clamp-2 text-ellipsis').text.strip()\n",
    "            product_category = 'Beauty & Cosmetics'\n",
    "            product_subcategory = link\n",
    "            try:\n",
    "                product_availability = grocery.find('p',class_='text-naivas-red text-sm font-bold').text.strip()\n",
    "            except Exception as e:\n",
    "                product_availability = np.nan\n",
    "            try:\n",
    "                final_price = grocery.find('span',class_='font-bold text-naivas-green mb-1 md:mb-0 pe-2').text.strip()\n",
    "            except Exception as e:\n",
    "                final_price = np.nan\n",
    "            last_scraped = datetime.now()\n",
    "            try:\n",
    "                initial_price = grocery.find('span',class_='text-red-600 text-xs line-through font-light').text.strip()\n",
    "            except Exception as e:\n",
    "                initial_price = np.nan\n",
    "\n",
    "            # Append Data to DataFrame\n",
    "            data = pd.DataFrame({\n",
    "                    'product_name':[product_name],\n",
    "                    'product_category':[product_category],\n",
    "                    'product_subcategory':[product_subcategory],\n",
    "                    'product_availability':[product_availability],\n",
    "                    'initial_price':[initial_price],\n",
    "                    'final_price':[final_price],\n",
    "                    'last_scraped':[last_scraped]\n",
    "                })\n",
    "\n",
    "            bigdata = pd.concat([bigdata,data])\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Handle Database Import Error\n",
    "    bigdata.to_csv('naivas_metadata.csv',index=False)\n",
    "    bigdata = pd.read_csv('naivas_metadata.csv')\n",
    "\n",
    "    # Drop Duplicate Records\n",
    "    bigdata['last_scraped'] = pd.to_datetime(bigdata['last_scraped'])\n",
    "\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(bigdata, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Define SQL Query to Retrieve Open Weather Data from Google Cloud BigQuery\n",
    "    sql = (\n",
    "           'SELECT *'\n",
    "           'FROM `cronjobs.naivas`'\n",
    "           )\n",
    "    \n",
    "    # Run SQL Query\n",
    "    data = client.query(sql).to_dataframe()\n",
    "\n",
    "    # Delete Original Table\n",
    "    client.delete_table(table_id)\n",
    "    print(f\"Table deleted successfully.\")\n",
    "\n",
    "    # Check Total Number of Duplicate Records\n",
    "    duplicated = data.duplicated(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price']).sum()\n",
    "    \n",
    "    # Remove Duplicate Records\n",
    "    data.drop_duplicates(subset=['product_name', 'product_category', 'product_subcategory', 'product_availability', 'initial_price', 'final_price'],inplace=True)\n",
    "\n",
    "    # Define the dataset ID and table ID\n",
    "    dataset_id = 'cronjobs'\n",
    "    table_id = 'naivas'\n",
    "    \n",
    "    # Define the table schema\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_category\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_subcategory\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"product_availability\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"initial_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"final_price\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"last_scraped\", \"TIMESTAMP\")\n",
    "    ]\n",
    "    \n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Create the table object\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    \n",
    "    # Create the table in BigQuery\n",
    "    table = client.create_table(table)\n",
    "    \n",
    "    print(f\"Table {table.table_id} created successfully.\")\n",
    "\n",
    "    # Define the BigQuery table ID\n",
    "    table_id = 'project-adrian-julius-aluoch.cronjobs.naivas'\n",
    "\n",
    "    # Load the data into the BigQuery table\n",
    "    job = client.load_table_from_dataframe(data, table_id)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while job.state != 'DONE':\n",
    "        time.sleep(2)\n",
    "        job.reload()\n",
    "        print(job.state)\n",
    "\n",
    "    # Success\n",
    "    print(f\"Data has been successfully retrieved, saved, and appended to the BigQuery table for {link}\")\n",
    "\n",
    "    # Remove Temporary File\n",
    "    os.remove('naivas_metadata.csv')\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataNerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
